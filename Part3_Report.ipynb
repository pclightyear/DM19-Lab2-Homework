{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3: Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Raw Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I convert the json raw data into pandas dataframe and concatenated the dataframe with the emotion label. The next step is to split the whole data set into train set and validation set. The validation set would not be used during the whole training process. It is only used for model validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some exploration, the next is to process the text data. Since I planned to utilize pre-trained glove twitter word embedding vectors, I tried to increase the cover rate by replacing some tokens that doesn't appears in the dictionary. For example, @user, hashtag, emojis, etc.\n",
    "\n",
    "1. Convert all text into lowercase.\n",
    "2. Delete <LH\\> token in the text.\n",
    "3. Utilize nltk tweet tokenizer to tokenizer text, it doesn't split token like @user or #hashtag into two different tokens.\n",
    "4. Replace @user token with \"<user\\>\" token. Also replace #something with \"<hashtag\\>\" and \"something\" two tokens.\n",
    "5. Replace common emojis with corresponding adjectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train naive bayes classifier as baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a baseline with naive bayes classifier. I only utilize top 50k unigrams and bigrams BOW features. The naive bayes classifier scores .443 on validation set. (macro f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | precision | recall | f1-score | support  |\n",
    "|-----------:|----------:|-------:|---------:|---------:|\n",
    "|       anger|0.27       |0.38    |0.31      |      5982|\n",
    "|anticipation|0.60       |0.54    |0.57      |     37269|\n",
    "|     disgust|0.37       |0.55    |0.44      |     20866|\n",
    "|        fear|0.31       |0.53    |0.39      |      9447|\n",
    "|         joy|0.67       |0.56    |0.61      |     77286|\n",
    "|     sadness|0.48       |0.45    |0.47      |     29160|\n",
    "|    surprise|0.31       |0.30    |0.31      |      7343|\n",
    "|       trust|0.46       |0.44    |0.45      |     30982|\n",
    "||\n",
    "|    accuracy|           |        |0.51      |    218335|\n",
    "|   macro avg|0.43       |0.47    |0.44      |    218335|\n",
    "|weighted avg|0.54       |0.51    |0.52      |    218335|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN model with sampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step I tried is to build a RNN model. Since the training process may be time-consuming, I only sampled 100k tweets for experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-train word embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As metioned before, I used pre-trained glove twitter word embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use keras Tokenizer to tokenize text (num_words=50k).\n",
    "2. Use keras pad_sequences to pad the text (maxlen=30), i.e. use the first 30 words to represent the text.\n",
    "3. Create one-hot encoding for emotion label.\n",
    "4. Create word embedding matrix with pre-trained embeddings.\n",
    "5. Split the data into training set and testing set (test_size=0.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build RNN model and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My RNN model consist of the following layers:\n",
    "\n",
    "1. Word embeddings layer (not trainable).\n",
    "2. One layer CuDNNLSTM to speed up training process (192 hidden nodes).\n",
    "3. Softmax output layer.\n",
    "\n",
    "I used categorical cross entropy as loss function and adam as optimizer. As for metrics, since keras has removed f1-score from Metrics since 2.0 version, I needed to implement it with keras Callback. Also, the model was trained for 7 epochs with batch size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.val_f1s = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.validation_data[0])\n",
    "        y_pred = label_decode(label_encoder, y_pred)\n",
    "        y_true = label_decode(label_encoder, self.validation_data[1])\n",
    "        _val_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        print(classification_report(y_true=y_true, y_pred=y_pred))\n",
    "        print(_val_f1)\n",
    "    \n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests in different inner layer settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I tried to find the best setting for inner layer. I tried similar model like CuDNNGRU, bidirectional CuDNNLSTM, bidirectional CuDNNGRU, multi-layer CuDNNLSTM, etc.; or tried to tune parameters on different hidden nodes or batch size. It turned out that those trials didn't generate significant improvement. With only 100k tweets, the RNN model with single layer CuDNNLSTM scores .446 on validation set, a similar result compared to the baseline. (macro f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            |precision  |recall  | f1-score |support   |\n",
    "|-----------:|----------:|-------:|---------:|---------:|\n",
    "|       anger|       0.31|   0.30 |     0.31 |     5982 |\n",
    "|anticipation|       0.57|   0.54 |     0.55 |    37269 |\n",
    "|     disgust|       0.37|   0.43 |     0.40 |    20866 |\n",
    "|        fear|       0.49|   0.39 |     0.44 |     9447 |\n",
    "|         joy|       0.60|   0.69 |     0.64 |    77286 |\n",
    "|     sadness|       0.45|   0.44 |     0.45 |    29160 |\n",
    "|    surprise|       0.46|   0.23 |     0.31 |     7343 |\n",
    "|       trust|       0.45|   0.35 |     0.39 |    30982 |\n",
    "||\n",
    "|    accuracy|           |        |     0.52 |   218335 |\n",
    "|   macro avg|       0.46|   0.42 |     0.44 |   218335 |\n",
    "|weighted avg|       0.51|   0.52 |     0.51 |   218335 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train RNN model with whole data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After experiment on 100k tweets, I started to train the RNN model on the whole data set. With about 1M tweets, it took about 20 minutes to train 7 epochs on single GTX 1080. The model scores .503 on validation set, about 6% improvement compared to the baseline. (macro f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            |  precision|    recall|  f1-score|   support|\n",
    "|-----------:|----------:|---------:|---------:|---------:|\n",
    "|       anger|       0.62|      0.27|      0.37|      5982|\n",
    "|anticipation|       0.64|      0.62|      0.63|     37269|\n",
    "|     disgust|       0.47|      0.42|      0.44|     20866|\n",
    "|        fear|       0.66|      0.46|      0.54|      9447|\n",
    "|         joy|       0.62|      0.78|      0.69|     77286|\n",
    "|     sadness|       0.47|      0.58|      0.52|     29160|\n",
    "|    surprise|       0.70|      0.25|      0.37|      7343|\n",
    "|       trust|       0.63|      0.37|      0.47|     30982|\n",
    "||\n",
    "|    accuracy|           |          |      0.59|    218335|\n",
    "|   macro avg|       0.60|      0.47|      0.50|    218335|\n",
    "|weighted avg|       0.59|      0.59|      0.58|    218335|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result on private leaderboard is .47704\n",
    "\n",
    "![Snapshot](img/kaggle_private_scoreboard_snapshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensemble other features with current RNN model.\n",
    "    * score\n",
    "    * date\n",
    "    * hashtags\n",
    "* Try different deep learning model.\n",
    "    * Attention Mechanim\n",
    "    * BERT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Mining)",
   "language": "python",
   "name": "data_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
